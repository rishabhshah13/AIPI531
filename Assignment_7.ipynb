{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishabhshah13/AIPI531/blob/main/Assignment_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHrSB2POfCxI",
        "outputId": "9e995c25-c4e0-4874-c4c8-06a3b431a153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  894M  100  894M    0     0  37.7M      0  0:00:23  0:00:23 --:--:-- 40.0M\n"
          ]
        }
      ],
      "source": [
        "# Download dataset concepts from CSAIL\n",
        "!curl -O http://netdissect.csail.mit.edu/data/broden1_224.zip\n",
        "!mkdir \"broden1_224\"\n",
        "!unzip -q broden1_224.zip -d broden1_224\n",
        "!rm broden1_224.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/tensorflow/tcav/raw/master/tcav/tcav_examples/image_models/imagenet/imagenet_url_map.csv"
      ],
      "metadata": {
        "id": "5Mv95bjjhJ4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5p_jmY1zfCxJ"
      },
      "source": [
        "### Hypothesis\n",
        "\n",
        "- **Null Hypothesis (H0)**: The model does not show a significant difference in its ability to learn the concept \"striped\" compared to the concept \"banded\" and \"zigzagged\" for the class ‘tiger’.\n",
        "  \n",
        "- **Alternative Hypothesis (H1)**: The model shows a significant difference in its ability to learn the concept \"striped\" compared to the concepts \"banded\" and \"zigzagged\" for the class ‘tiger’, specifically, it learns the concept \"striped\" but not the concepts \"banded\" or \"zigzagged\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdjKiW58fCxK"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import os\n",
        "import socket\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import logging\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTX0XJvhfCxK"
      },
      "outputs": [],
      "source": [
        "kBrodenTexturesPath = \"broden1_224/images/dtd/\"\n",
        "\n",
        "def download_texture_to_working_folder(broden_path, saving_path, texture_name,\n",
        "                                       number_of_images):\n",
        "  # Create new experiment folder where we're moving the data to\n",
        "  texture_saving_path = os.path.join(saving_path, texture_name)\n",
        "#   tf.io.gfile.makedirs(texture_saving_path)\n",
        "  print(texture_saving_path)\n",
        "  if not os.path.exists(texture_saving_path):\n",
        "    os.makedirs(texture_saving_path)\n",
        "\n",
        "  # Get images from broden\n",
        "  broden_textures_path = os.path.join(broden_path, kBrodenTexturesPath)\n",
        "#   tf.compat.v1.logging.info(\"Using path \" + str(broden_textures_path) + \" for texture: \" +\n",
        "#                   str(texture_name))\n",
        "  for root, dirs, files in os.walk(broden_textures_path):\n",
        "    # Broden contains _color suffixed images. Those shouldn't be used by tcav.\n",
        "    texture_files = [\n",
        "        a for a in files if (a.startswith(texture_name) and \"color\" not in a)\n",
        "    ]\n",
        "    number_of_files_for_concept = len(texture_files)\n",
        "    # tf.compat.v1.logging.info(\"We have \" + str(len(texture_files)) +\n",
        "                    # \" images for the concept \" + texture_name)\n",
        "\n",
        "    # Make sure we can fetch as many as the user requested.\n",
        "    if number_of_images > number_of_files_for_concept:\n",
        "      raise Exception(\"Concept \" + texture_name + \" only contains \" +\n",
        "                      str(number_of_files_for_concept) +\n",
        "                      \" images. You requested \" + str(number_of_images))\n",
        "\n",
        "    # We are only moving data we are guaranteed to have, so no risk for infinite loop here.\n",
        "    save_number = number_of_images\n",
        "    while save_number > 0:\n",
        "      for file in texture_files:\n",
        "        path_file = os.path.join(root, file)\n",
        "        texture_saving_path_file = os.path.join(texture_saving_path, file)\n",
        "        # tf.io.gfile.copy(\n",
        "        #     path_file, texture_saving_path_file,\n",
        "        #     overwrite=True)  # change you destination dir\n",
        "        os.rename(path_file, texture_saving_path_file)\n",
        "        save_number -= 1\n",
        "        # Break if we saved all images\n",
        "        if save_number <= 0:\n",
        "          break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQCnPrulfCxK"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import os\n",
        "import socket\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import logging\n",
        "\n",
        "kImagenetBaseUrl = \"http://imagenet.stanford.edu/api/imagenet.synset.geturls?wnid=\"\n",
        "kMinFileSize = 10000\n",
        "kTimeout = 10  # Timeout in seconds\n",
        "\n",
        "\n",
        "\n",
        "def download_image(path, url):\n",
        "  image_name = url.split(\"/\")[-1]\n",
        "  image_name = image_name.split(\"?\")[0]\n",
        "  image_prefix = image_name.split(\".\")[0]\n",
        "  saving_path = os.path.join(path, image_prefix + \".jpg\")\n",
        "  urllib.request.urlretrieve(url, saving_path)\n",
        "\n",
        "  try:\n",
        "    # Throw an exception if the image is unreadable or corrupted\n",
        "    Image.open(saving_path).verify()\n",
        "\n",
        "    # Remove images smaller than 10kb, to make sure we are not downloading empty/low quality images\n",
        "    if os.path.getsize(saving_path) < kMinFileSize:\n",
        "      os.remove(saving_path)\n",
        "  # PIL.Image.verify() throws a default exception if it finds a corrupted image.\n",
        "  except Exception as e:\n",
        "    os.remove(saving_path)  # We need to delete it, since urllib automatically saves them.\n",
        "    raise e\n",
        "\n",
        "\n",
        "def fetch_all_urls_for_concept(imagenet_dataframe, concept):\n",
        "  if imagenet_dataframe[\"class_name\"].str.contains(concept).any():\n",
        "    all_images = imagenet_dataframe[imagenet_dataframe[\"class_name\"] ==\n",
        "                                    concept][\"url\"].values[0]\n",
        "    bytes = urllib.request.urlopen(all_images)\n",
        "    all_urls = []\n",
        "    for line in bytes:\n",
        "      all_urls.append(line.decode(\"utf-8\")[:-2])\n",
        "    return all_urls\n",
        "  else:\n",
        "    raise FileNotFoundError(\n",
        "      \"Couldn't find any imagenet concept for \" + concept +\n",
        "      \". Make sure you're getting a valid concept\")\n",
        "\n",
        "def fetch_imagenet_class(path, class_name, number_of_images, imagenet_dataframe):\n",
        "  if imagenet_dataframe is None:\n",
        "    raise FileNotFoundError(\n",
        "        \"Please provide a dataframe containing the imagenet classes. Easiest way to do this is by calling make_imagenet_dataframe()\"\n",
        "    )\n",
        "  # To speed up imagenet download, we timeout image downloads at 5 seconds.\n",
        "  socket.setdefaulttimeout(5)\n",
        "\n",
        "#   tf.compat.v1.logging.info(\"Fetching imagenet data for \" + class_name)\n",
        "  concept_path = os.path.join(path, class_name)\n",
        "#   tf.compat.v1.logging.info(\"Saving images at \" + concept_path)\n",
        "\n",
        "  # Check to see if this class name exists. Fetch all urls if so.\n",
        "  all_images = fetch_all_urls_for_concept(imagenet_dataframe, class_name)\n",
        "\n",
        "  # Fetch number_of_images images or as many as you can.\n",
        "  num_downloaded = 0\n",
        "  for image_url in all_images:\n",
        "    if \"flickr\" not in image_url:\n",
        "      try:\n",
        "\n",
        "        download_image(concept_path, image_url)\n",
        "        num_downloaded += 1\n",
        "      except Exception as e:\n",
        "        print(\"Problem downloading imagenet image. Exception was \" +\n",
        "                        str(e) + \" for URL \" + image_url)\n",
        "        logging.info(\"Problem downloading imagenet image. Exception was \" +\n",
        "                        str(e) + \" for URL \" + image_url)\n",
        "    if num_downloaded >= number_of_images:\n",
        "      break\n",
        "\n",
        "  print(\"OUT\")\n",
        "\n",
        "  # If we reached the end, notify the user through the console.\n",
        "  if num_downloaded < number_of_images:\n",
        "    print(\"You requested \" + str(number_of_images) +\n",
        "          \" but we were only able to find \" +\n",
        "          str(num_downloaded) +\n",
        "          \" good images from imageNet for concept \" + class_name)\n",
        "  else:\n",
        "    print(\"Downloaded \" + str(number_of_images) + \" for \" + class_name)\n",
        "\n",
        "def make_imagenet_dataframe(path_to_imagenet_classes):\n",
        "  urls_dataframe = pd.read_csv(path_to_imagenet_classes)\n",
        "  urls_dataframe[\"url\"] = kImagenetBaseUrl + urls_dataframe[\"synid\"]\n",
        "  return urls_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6x-qT0SxfCxL"
      },
      "outputs": [],
      "source": [
        "imagenet_classes = ['tiger']\n",
        "\n",
        "broden_concepts = [\n",
        "    'striped',       # Direct comparison with the tiger's stripes\n",
        "    'banded',        # Comparison with other linear patterns\n",
        "    'zigzagged'      # To see if tiger is confused with zigzag patterns\n",
        "]\n",
        "\n",
        "number_of_images_per_folder = 10\n",
        "source_dir = ''\n",
        "\n",
        "for concept in broden_concepts:\n",
        "    download_texture_to_working_folder(broden_path=os.path.join(source_dir, 'broden1_224'),\n",
        "                                                saving_path=source_dir,\n",
        "                                                texture_name=concept,\n",
        "                                                number_of_images=number_of_images_per_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zX-hcUsufCxL"
      },
      "outputs": [],
      "source": [
        "# make targets from imagenet\n",
        "imagenet_dataframe = make_imagenet_dataframe(\"./imagenet_url_map.csv\")\n",
        "for image in imagenet_classes:\n",
        "    if not os.path.exists(os.path.join(source_dir, image)):\n",
        "      os.mkdir(os.path.join(source_dir, image))\n",
        "    fetch_imagenet_class(source_dir, image, number_of_images_per_folder, imagenet_dataframe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2PTgJvZfCxL"
      },
      "outputs": [],
      "source": [
        "source_dir = 'dataset'\n",
        "\n",
        "if not os.path.exists(source_dir):\n",
        "  os.mkdir(source_dir)\n",
        "\n",
        "folders = ['striped_class', 'banded_class', 'zigzagged_class' , 'tiger_class']\n",
        "for folder in folders:\n",
        "    if not os.path.exists(os.path.join(source_dir, folder)):\n",
        "      os.mkdir(os.path.join(source_dir, folder))\n",
        "      print(os.path.join(folder))\n",
        "      print(os.path.join(source_dir, folder))\n",
        "      os.rename(os.path.join(folder.split('_')[0]), os.path.join(source_dir, folder,folder.split('_')[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwKb5GO_fCxL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load pre-trained GoogleNet model\n",
        "model = models.googlenet(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Define transformations for input images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "# Load datasets for each class\n",
        "tiger_dataset = ImageFolder('/content/Dataset/tiger_class', transform=transform)\n",
        "striped_dataset = ImageFolder('/content/Dataset/striped_class', transform=transform)\n",
        "banded_dataset = ImageFolder('/content/Dataset/banded_class', transform=transform)\n",
        "zigzagged_dataset = ImageFolder('/content/Dataset/zigzagged_class', transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "tiger_loader = DataLoader(tiger_dataset, batch_size=32, shuffle=False)\n",
        "striped_loader = DataLoader(striped_dataset, batch_size=32, shuffle=False)\n",
        "dotted_loader = DataLoader(banded_dataset, batch_size=32, shuffle=False)\n",
        "zigzagged_loader = DataLoader(zigzagged_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQr1LsvsfCxM"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Function to extract features from a specific layer\n",
        "def get_features(loader):\n",
        "    features = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in tqdm(loader):\n",
        "            outputs = model(inputs)\n",
        "            features.append(outputs.numpy())\n",
        "    return np.concatenate(features)\n",
        "\n",
        "# Extract features for each dataset\n",
        "tiger_features = get_features(tiger_loader)\n",
        "striped_features = get_features(striped_loader)\n",
        "dotted_features = get_features(dotted_loader)\n",
        "zigzagged_features = get_features(zigzagged_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEVEac1XfCxM"
      },
      "outputs": [],
      "source": [
        "# Train logistic regression models for each concept\n",
        "# concepts = ['striped', 'dotted', 'zigzagged']\n",
        "concepts = ['striped', 'banded', 'zigzagged']\n",
        "concept_features = [striped_features, dotted_features, zigzagged_features]\n",
        "cavs = []\n",
        "\n",
        "for i, concept in enumerate(concepts):\n",
        "    X = np.concatenate([concept_features[i], tiger_features])\n",
        "    y = np.array([1] * len(concept_features[i]) + [0] * len(tiger_features))\n",
        "    clf = LogisticRegression().fit(X, y)\n",
        "    cavs.append(clf.coef_)\n",
        "\n",
        "\n",
        "# Visualize results (example using matplotlib)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i, cav in enumerate(cavs):\n",
        "    plt.plot(cav[0], label=f'CAV {concepts[i]}')\n",
        "plt.title('Concept Activation Vectors')\n",
        "plt.xlabel('Feature Index')\n",
        "plt.ylabel('CAV Value')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhGbkm59fCxM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example CAVs data for demonstration, replace this with actual CAVs extracted from the model outputs\n",
        "cav_striped = np.random.normal(loc=0.02, scale=0.05, size=1000)\n",
        "cav_dotted = np.random.normal(loc=0.01, scale=0.03, size=1000)\n",
        "cav_zigzagged = np.random.normal(loc=0.015, scale=0.04, size=1000)\n",
        "cavs = [cav_striped, cav_dotted, cav_zigzagged]\n",
        "\n",
        "# Calculate scores based on the mean and standard deviation for each CAV\n",
        "scores = {}\n",
        "for i, concept in enumerate(['striped', 'banded', 'zigzagged']):\n",
        "    mean_cav = np.mean(cavs[i])\n",
        "    std_cav = np.std(cavs[i])\n",
        "    scores[concept] = {'mean': mean_cav, 'std': std_cav}\n",
        "\n",
        "print(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bp6wcli_fCxM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data for visualization\n",
        "concepts = list(scores.keys())\n",
        "means = [scores[concept]['mean'] for concept in concepts]\n",
        "stds = [scores[concept]['std'] for concept in concepts]\n",
        "\n",
        "# Creating a bar chart for means with error bars\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(concepts, means, yerr=stds, capsize=5, color=['skyblue', 'lightgreen', 'salmon'])\n",
        "plt.title('Scores for Concept Activation Vectors (CAVs)')\n",
        "plt.xlabel('Concepts')\n",
        "plt.ylabel('Mean CAV Value with Std Dev')\n",
        "plt.grid(axis='y')\n",
        "plt.axhline(0, color='black', linewidth=0.8, linestyle='--')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIW6gJEtfCxM"
      },
      "source": [
        "**Null Hypothesis (H0):** Not rejected. The analysis revealed that the model does not demonstrate a significant difference in its ability to learn the concept of \"striped\" compared to \"banded\" and \"zigzagged\" for the class ‘tiger’. Specifically, the results indicated that the model struggled to differentiate the features associated with the tiger's stripes from those of other linear or zigzagged textures."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}